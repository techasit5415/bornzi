"""
Main Streamlit Application
PDF RAG Chatbot with Gemma2:27b
"""

import streamlit as st
import os

# Import custom modules
from embeddings_config import EmbeddingFactory
from pdf_processor import process_pdf, load_vectorstore, save_vectorstore
# from llm_config import create_qa_chain, get_answer
from llm_config import create_qa_chain, get_answer
from ui_components import render_sidebar, render_controls, render_instructions, display_source_documents


# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Page
st.set_page_config(page_title="PDF RAG Chatbot", page_icon="üìö", layout="wide")
st.title("üìö PDF RAG Chatbot with Gemma2:27b")
st.markdown("---")

# Initialize session state
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'pdf_processed' not in st.session_state:
    st.session_state.pdf_processed = False
if 'current_pdf_name' not in st.session_state:
    st.session_state.current_pdf_name = None
if 'embedding_model' not in st.session_state:
    st.session_state.embedding_model = "gemma2:27b"  # ‡πÉ‡∏ä‡πâ DeepSeek ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

# Constants
VECTORSTORE_DIR = "vectorstore_cache"
os.makedirs(VECTORSTORE_DIR, exist_ok=True)

# ==================== SIDEBAR ====================
with st.sidebar:
    # Render sidebar components
    uploaded_file, model_info = render_sidebar(VECTORSTORE_DIR)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Vector Store ‡πÄ‡∏Å‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if uploaded_file is not None:
        vectorstore_path = os.path.join(VECTORSTORE_DIR, f"{uploaded_file.name}.faiss")
        
        # ‡∏õ‡∏∏‡πà‡∏°‡πÇ‡∏´‡∏•‡∏î Vector Store ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        if os.path.exists(vectorstore_path) and not st.session_state.pdf_processed:
            if st.button("‚ö° ‡πÇ‡∏´‡∏•‡∏î Vector Store ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà", type="secondary"):
                with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Vector Store..."):
                    try:
                        st.session_state.vectorstore = load_vectorstore(
                            vectorstore_path,
                            st.session_state.embedding_model
                        )
                        st.session_state.pdf_processed = True
                        st.session_state.chat_history = []
                        st.session_state.current_pdf_name = uploaded_file.name
                        st.success(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î Vector Store ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                        st.info(f"ü§ñ Model: {model_info['name']}")
                        st.rerun()
                    except Exception as e:
                        st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {str(e)}")
    
    # ‡∏õ‡∏∏‡πà‡∏° Process PDF
    if uploaded_file is not None:
        if st.button("üîÑ Process PDF", type="primary"):
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF..."):
                try:
                    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF
                    result = process_pdf(uploaded_file, st.session_state.embedding_model)
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    st.session_state.vectorstore = result['vectorstore']
                    st.session_state.pdf_processed = True
                    st.session_state.chat_history = []
                    st.session_state.current_pdf_name = uploaded_file.name
                    
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Vector Store
                    vectorstore_path = os.path.join(VECTORSTORE_DIR, f"{uploaded_file.name}.faiss")
                    save_vectorstore(result['vectorstore'], vectorstore_path)
                    
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                    st.success(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                    st.info(f"üìÑ ‡πÑ‡∏ü‡∏•‡πå: {uploaded_file.name}")
                    st.info(result['chunk_info'])
                    st.info(f"üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥: {result['num_pages']} ‡∏´‡∏ô‡πâ‡∏≤ | {result['total_chars']:,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ | {result['num_chunks']} chunks")
                    
                    current_model_info = EmbeddingFactory.get_model_info(st.session_state.embedding_model)
                    st.info(f"ü§ñ Model: {current_model_info['name']} (‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á Embedding & LLM)")
                    
                    # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ model
                    if result['recommended_model'] != st.session_state.embedding_model:
                        rec_info = EmbeddingFactory.get_model_info(result['recommended_model'])
                        st.warning(f"üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {rec_info['name']} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ç‡∏ô‡∏≤‡∏î‡∏ô‡∏µ‡πâ")
                    
                    st.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: `{vectorstore_path}`")
                    
                except Exception as e:
                    st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏•‡∏∞‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°
    render_instructions()
    render_controls()


# ==================== MAIN CHAT INTERFACE ====================
if not st.session_state.pdf_processed:
    st.info("üëà ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ Upload ‡πÅ‡∏•‡∏∞ Process ‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
else:
    # ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.chat_history:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
    
    # Input ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£..."):
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        with st.chat_message("assistant"):
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."):
                try:
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á QA Chain ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ model ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô sidebar
                    qa_chain = create_qa_chain(
                        st.session_state.vectorstore,
                        llm_model=st.session_state.embedding_model  # ‡πÉ‡∏ä‡πâ model ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                    )
                    
                    # ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                    response = get_answer(qa_chain, prompt)
                    answer = response['result']
                    
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                    st.markdown(answer)
                    
                    # ‡πÅ‡∏™‡∏î‡∏á source documents
                    display_source_documents(response)
                    
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
                    st.session_state.chat_history.append({"role": "assistant", "content": answer})
                    
                except Exception as e:
                    error_msg = f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"
                    st.error(error_msg)
                    st.session_state.chat_history.append({"role": "assistant", "content": error_msg})


# ==================== FOOTER ====================
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>ü§ñ Powered by Gemma2:27b (Ollama) | ü¶ú LangChain | üéà Streamlit</p>
</div>
""", unsafe_allow_html=True)
